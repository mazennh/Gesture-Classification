{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4275761,"sourceType":"datasetVersion","datasetId":2466069}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Configuration & Imports","metadata":{}},{"cell_type":"code","source":"# 1. Remove old folder if it exists\n!rm -rf YourRepoName\n\n# 2. Clone your code\n!git clone https://github.com/mazennh/Gesture-Classification.git\n\n# 3. Install dependencies\n!pip install -r /kaggle/working/Gesture-Classification/requirements.txt --quiet\n\n# 4. Allow Python to find your files\nimport sys\nimport os\nsys.path.append(os.path.abspath(\"/kaggle/working/Gesture-Classification\"))\n\n# 5. Auto-reload (Optional: helps if you edit code and git pull)\n%load_ext autoreload\n%autoreload 2\n\nprint(\"Setup Complete! Your custom modules are ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport subprocess\nfrom torchinfo import summary\nimport warnings\nfrom torch.optim.lr_scheduler import StepLR\nfrom pyngrok import ngrok\nimport data_utils\nimport model_utils\nimport train_utils\nimport vis_utils\n\nwarnings.filterwarnings(\"ignore\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Data Pipeline Construction","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Dataset Filtering & Splitting","metadata":{}},{"cell_type":"code","source":"data_utils.filter_data(input_path = '/kaggle/input/hagrid-classification-512p/hagrid-classification-512p',\n           output_path = '/kaggle/working/filtered_data',\n           split_path = '/kaggle/working/splited_data',\n           classes_list = [\"stop\", \"dislike\", \"like\",\n                           \"peace\", \"peace_inverted\", \"ok\",\n                           \"call\", \"mute\", \"stop_inverted\"],\n           split_ratio = (0.8,0.1,0.1),\n           seed = 42\n          )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Class Distribution Analysis\n\nthe dataset is well-balanced across all target classes. Since there is no significant class imbalance, no oversampling or class-weighting techniques are required","metadata":{}},{"cell_type":"code","source":"data_utils.class_distribution(root_path = '/kaggle/working/filtered_data')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_utils.class_distribution(root_path = '/kaggle/working/splited_data/train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_utils.class_distribution(root_path = '/kaggle/working/splited_data/val')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_utils.class_distribution(root_path = '/kaggle/working/splited_data/test')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2.2 Sample Visualization & Resolution Check\nAnalysis of random samples indicates that most images have a resolution of **512×512**. However, our target architectures **(VGG16, ResNet, InceptionV1, ViT)** generally are optimized for input dimensions of **224×224**.","metadata":{}},{"cell_type":"code","source":"vis_utils.visualize_random_samples(root_path = \"/kaggle/working/filtered_data\",\n                                   n_samples=10,\n                                   cols = 4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Image Augmentation & Preprocessing\n\n### Train\n* RandomResizedCrop **(Resize -> 224,224)**\n* RandomHorizontalFlip\n* RandomRotation **(5° to 15°)**\n* Brightness/Contrast **(ColorJitter)**\n* ToTensor\n* Normalize\n\n### Test & Val\n\n* Turn data into **tensors**\n* Normalization\n* Resize **(224,224)** ","metadata":{}},{"cell_type":"code","source":"dls = data_utils.create_dataloaders(\n    data_dir=\"/kaggle/working/splited_data\",\n    batch_size=32,\n    img_size=224\n)\n\ntrain_dataloader, val_dataloader, test_dataloader, train_dataset, class_names, class_dict = dls\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.4 Data Verification\n\n#### Sanity Check\n\nBefore feeding data into the model, we perform a final sanity check. We retrieve a single batch from the DataLoader, reverse the normalization, and visualize the images. This ensures that our augmentation pipeline is functioning correctly and that labels match the image content.","metadata":{}},{"cell_type":"code","source":"img, _ = next(iter(train_dataloader))\nimg.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vis_utils.data_verification(dataset = train_dataset,\n                            class_names = class_names,\n                            n_rows=3,\n                            n_cols=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Model","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Making Model","metadata":{}},{"cell_type":"code","source":"model, architecture_name = model_utils.get_model(num_classes=len(class_names),\n                                                 model_name='resnet',\n                                                 device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Model Summary","metadata":{}},{"cell_type":"code","source":"img, _ = next(iter(train_dataloader))\nimg.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(model,input_size=(img.shape))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4 Model Training","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Loss Function & Optimizer","metadata":{}},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2 Model Training","metadata":{}},{"cell_type":"code","source":"model_train=train_utils.train(model=model,\n                              train_dataloader=train_dataloader,\n                              val_dataloader=val_dataloader,\n                              optimizer=optimizer,\n                              loss_fn=loss_fn,\n                              num_classes = len(class_names),\n                              best_model = architecture_name,\n                              scheduler=scheduler,\n                              device=device,\n                              patience=10,\n                              experiment_name = architecture_name,\n                              epochs=100)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}